{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d8158dfddb2ac72",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "In this assignment, you will continue with the Bigram Language Model from the Lecture. Make the training loop and inference for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c011cbf15834af26",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "project_path = \"C:\\\\Users\\\\eunse\\\\OneDrive\\\\바탕 화면\\\\LLM101n\"\n",
    "sys.path.insert(0, project_path)\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from utils import load_text, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a0b5274f4dca2",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1253f6800c36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BigramConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "\n",
    "    seed: int = 101\n",
    "    \n",
    "config = BigramConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726a09b7e375389",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e4d38445588cdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 101\n"
     ]
    }
   ],
   "source": [
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74be7e01434a14b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e86ea6f15f11b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from c:\\Users\\eunse\\OneDrive\\바탕 화면\\LLM101n\\notebooks\\Assignments/../../data/names.txt (length: 228145 characters).\n"
     ]
    }
   ],
   "source": [
    "names = load_text(config.root_dir + config.dataset_path).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff6f7e1bbade4",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed24c0db0ce9da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special token\n",
    "names = [\".\" + name + \".\" for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e38655c11342dd",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a2f768e619dfb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "config.vocab_size = len(chars)\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4e1dda633584d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "523dd8edb6b3e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "W = torch.randn(config.vocab_size, config.vocab_size, requires_grad=True)\n",
    "b = torch.randn(config.vocab_size, requires_grad=True)\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca7979aafb68a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6125c93c9c7519",
   "metadata": {},
   "source": [
    "#### Task 1: Train Bigram Language Model (Neural Network Approach)\n",
    "\n",
    "Make the training loop for the Bigram Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f5165e72e37f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of Input, Target pairs\n",
    "inputs, targets = [], []\n",
    "for name in names:\n",
    "    for char1, char2 in zip(name, name[1:]):\n",
    "        input = str2idx[char1]\n",
    "        target = str2idx[char2]\n",
    "        inputs.append(input)\n",
    "        targets.append(target)\n",
    "\n",
    "# Convert to tensor\n",
    "inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe57ba03f098d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Input, Target pairs: 228146\n",
      "Input shape: torch.Size([228146])\n",
      "Target shape: torch.Size([228146])\n",
      "First (Input, Target): (0, 5)\n",
      "Second (Input, Target): (5, 13)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Input, Target pairs: {len(inputs)}\")\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "print(f\"First (Input, Target): ({inputs[0]}, {targets[0]})\")\n",
    "print(f\"Second (Input, Target): ({inputs[1]}, {targets[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baab50fd5515e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# One-hot encode the input tensor.                                             #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "inputs_encoded = F.one_hot(inputs, num_classes = 27)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Convert data type to float\n",
    "inputs_encoded = inputs_encoded.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ef07b0b820e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "steps = 100\n",
    "lr = 10\n",
    "\n",
    "for step in range(1, steps + 1):\n",
    "    # Forward pass\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Implement the forward pass.                                                  #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    logits = inputs_encoded @ W + b\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    # loss\n",
    "    log_probs = torch.log(probs + 1e-9)  # Add small value to prevent log(0)\n",
    "    loss = -log_probs[torch.arange(len(targets)), targets].mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Implement the backward pass.                                                 #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    loss.backward()\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    # Update weights\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Update the weights of the model using the gradients.                         #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    with torch.no_grad():\n",
    "        W -= lr * W.grad\n",
    "        b -= lr * b.grad\n",
    "\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d956fef0a027963",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f852b486b92cb",
   "metadata": {},
   "source": [
    "#### Task 2: Generate a Name\n",
    "\n",
    "Create a function to generate a name using the trained Bigram Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31dfacd08b51cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate a name\n",
    "def generate_name():\n",
    "    new_name = []\n",
    "    start_idx = str2idx[\".\"]\n",
    "    \n",
    "    while True:\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # 1. Forward pass                                                              #\n",
    "        # 2. Sample the next token                                                     #\n",
    "        # 3. Decode the token                                                          #\n",
    "        # 4. Update the start_idx                                                      #\n",
    "        # 5. Break if the next character is \".\"                                        #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        # Forward pass\n",
    "        x_encoded = F.one_hot(torch.tensor([start_idx]), num_classes = config.vocab_size).float()\n",
    "        logits = x_encoded @ W + b\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        # Sample\n",
    "        next_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        # Decode\n",
    "        next_char = idx2str[next_idx]\n",
    "        \n",
    "        # Update\n",
    "        start_idx = next_idx\n",
    "        \n",
    "        # Break if \".\"\n",
    "        if next_char == \".\":\n",
    "            break\n",
    "        \n",
    "        new_name.append(next_char)\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return ''.join(new_name)\n",
    "\n",
    "# Generate 5 names\n",
    "for _ in range(5):\n",
    "    print(generate_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d00faaddbe4b44",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "\n",
    "We have already made our own custom auto-grad Tensor class. Let's use it!\n",
    "\n",
    "Train the Bigram Language Model using our custom auto-grad Tensor class.\n",
    "\n",
    "**Do not use any built-in PyTorch functions.** (other deep learning libraries are also prohibited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f26efc1212bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, _children=(), _operation=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self.gradient = 0\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"tensor=({self.data})\"\n",
    "\n",
    "    def __add__(self, other):  # self + other\n",
    "        output = Tensor(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.gradient = 1 * output.gradient\n",
    "            other.gradient = 1 * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):  # self * other\n",
    "        output = Tensor(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.gradient = other.data * output.gradient\n",
    "            other.gradient = self.data * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def tanh(self):  # tanh(self)\n",
    "        output = Tensor(math.tanh(self.data), (self,), 'tanh')\n",
    "        def _backward():\n",
    "            self.gradient = (1.0 - math.tanh(self.data) ** 2) * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __pow__(self, power):  # self ** power\n",
    "        assert isinstance(power, (int, float)), \"Power must be an int or a float\"\n",
    "        output = Tensor(self.data ** power, (self,), f'**{power}')\n",
    "        def _backward():\n",
    "            self.gradient = power * (self.data ** (power - 1)) * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.gradient = 1\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * Tensor(-1.0)\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7815ada66df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
